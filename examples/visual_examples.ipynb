{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Regression: Visual Examples\n",
    "\n",
    "This notebook demonstrates the main features of the `kernel-regression` package with visual examples.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Basic Nadaraya-Watson Regression](#1-basic-nadaraya-watson-regression)\n",
    "2. [Local Polynomial Regression](#2-local-polynomial-regression)\n",
    "3. [Boundary Bias Comparison](#3-boundary-bias-comparison)\n",
    "4. [Bandwidth Selection](#4-bandwidth-selection)\n",
    "5. [Per-Dimension Bandwidth (Variable Selection)](#5-per-dimension-bandwidth-variable-selection)\n",
    "6. [Heteroscedasticity Detection](#6-heteroscedasticity-detection)\n",
    "7. [Confidence Intervals](#7-confidence-intervals)\n",
    "8. [Goodness of Fit Diagnostics](#8-goodness-of-fit-diagnostics)\n",
    "9. [2D Regression Visualization](#9-2d-regression-visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend for automated execution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import kernel regression package\n",
    "from kernel_regression import (\n",
    "    NadarayaWatson,\n",
    "    LocalPolynomialRegression,\n",
    "    CrossValidatedBandwidth,\n",
    "    GoodnessOfFit,\n",
    "    heteroscedasticity_test,\n",
    "    wild_bootstrap_confidence_intervals,\n",
    "    fan_yao_variance_estimation,\n",
    ")\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Nadaraya-Watson Regression\n",
    "\n",
    "Nadaraya-Watson (local constant) kernel regression is the simplest form of kernel regression. It computes weighted averages of nearby points using a kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 4*np.pi, 100).reshape(-1, 1)\n",
    "y_true = np.sin(X.ravel())\n",
    "y = y_true + 0.3 * np.random.randn(100)\n",
    "\n",
    "# Fit Nadaraya-Watson with automatic bandwidth selection\n",
    "model = NadarayaWatson(bandwidth=\"cv\").fit(X, y)\n",
    "\n",
    "# Predict on fine grid\n",
    "X_plot = np.linspace(0, 4*np.pi, 300).reshape(-1, 1)\n",
    "y_pred = model.predict(X_plot)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(X, y, alpha=0.5, s=30, label='Training data')\n",
    "plt.plot(X_plot, np.sin(X_plot.ravel()), 'g--', linewidth=2, label='True function', alpha=0.7)\n",
    "plt.plot(X_plot, y_pred, 'r-', linewidth=2.5, label=f'NW prediction (h={model.bandwidth_[0]:.3f})')\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Nadaraya-Watson Kernel Regression with CV Bandwidth', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Selected bandwidth: {model.bandwidth_[0]:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Local Polynomial Regression\n",
    "\n",
    "Local polynomial regression fits a polynomial at each point using weighted least squares. Higher-order polynomials can capture more complex local patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate data with more complex pattern\n",
    "np.random.seed(123)\n",
    "X = np.linspace(0, 10, 80).reshape(-1, 1)\n",
    "y_true = np.sin(X.ravel()) * np.exp(-X.ravel()/10)\n",
    "y = y_true + 0.15 * np.random.randn(80)\n",
    "\n",
    "# Fit models with different polynomial orders\n",
    "orders = [0, 1, 2]\n",
    "models = {}\n",
    "for order in orders:\n",
    "    if order == 0:\n",
    "        # Order 0 is Nadaraya-Watson\n",
    "        models[order] = NadarayaWatson(bandwidth=0.5).fit(X, y)\n",
    "    else:\n",
    "        models[order] = LocalPolynomialRegression(order=order, bandwidth=0.5).fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_plot = np.linspace(0, 10, 300).reshape(-1, 1)\n",
    "y_plot_true = np.sin(X_plot.ravel()) * np.exp(-X_plot.ravel()/10)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(X, y, alpha=0.5, s=30, c='gray', label='Training data')\n",
    "plt.plot(X_plot, y_plot_true, 'k--', linewidth=2, label='True function', alpha=0.7)\n",
    "\n",
    "colors = ['orange', 'red', 'purple']\n",
    "labels = ['Order 0 (Nadaraya-Watson)', 'Order 1 (Local Linear)', 'Order 2 (Local Quadratic)']\n",
    "for i, order in enumerate(orders):\n",
    "    y_pred = models[order].predict(X_plot)\n",
    "    plt.plot(X_plot, y_pred, color=colors[i], linewidth=2, label=labels[i])\n",
    "\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Local Polynomial Regression: Effect of Polynomial Order', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boundary Bias Comparison\n",
    "\n",
    "Nadaraya-Watson suffers from boundary bias because it can only average points on one side near boundaries. Local polynomial regression (especially local linear) eliminates this bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate data on bounded interval\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "y = X.ravel()  # True function: y = x\n",
    "y_noisy = y + 0.05 * np.random.randn(100)\n",
    "\n",
    "# Fit both models\n",
    "nw_model = NadarayaWatson(bandwidth=0.1).fit(X, y_noisy)\n",
    "lp_model = LocalPolynomialRegression(order=1, bandwidth=0.1).fit(X, y_noisy)\n",
    "\n",
    "# Predict across the range\n",
    "X_plot = np.linspace(0, 1, 300).reshape(-1, 1)\n",
    "nw_pred = nw_model.predict(X_plot)\n",
    "lp_pred = lp_model.predict(X_plot)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Full range\n",
    "axes[0].scatter(X, y_noisy, alpha=0.4, s=20, c='gray', label='Data')\n",
    "axes[0].plot(X_plot, X_plot.ravel(), 'g--', linewidth=2, label='True function (y=x)', alpha=0.7)\n",
    "axes[0].plot(X_plot, nw_pred, 'orange', linewidth=2.5, label='Nadaraya-Watson')\n",
    "axes[0].plot(X_plot, lp_pred, 'red', linewidth=2.5, label='Local Polynomial (order=1)')\n",
    "axes[0].set_xlabel('X', fontsize=12)\n",
    "axes[0].set_ylabel('y', fontsize=12)\n",
    "axes[0].set_title('Boundary Bias: Full Range', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Zoom on boundaries\n",
    "axes[1].scatter(X, y_noisy, alpha=0.4, s=20, c='gray', label='Data')\n",
    "axes[1].plot(X_plot, X_plot.ravel(), 'g--', linewidth=2, label='True function (y=x)', alpha=0.7)\n",
    "axes[1].plot(X_plot, nw_pred, 'orange', linewidth=2.5, label='Nadaraya-Watson')\n",
    "axes[1].plot(X_plot, lp_pred, 'red', linewidth=2.5, label='Local Polynomial (order=1)')\n",
    "axes[1].set_xlim([-0.05, 0.25])\n",
    "axes[1].set_ylim([-0.05, 0.25])\n",
    "axes[1].set_xlabel('X', fontsize=12)\n",
    "axes[1].set_ylabel('y', fontsize=12)\n",
    "axes[1].set_title('Boundary Bias: Left Boundary (Zoomed)', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute boundary bias\n",
    "boundary_x = np.array([[0.0], [1.0]])\n",
    "nw_boundary = nw_model.predict(boundary_x)\n",
    "lp_boundary = lp_model.predict(boundary_x)\n",
    "true_boundary = boundary_x.ravel()\n",
    "\n",
    "print(\"\\nBoundary Predictions:\")\n",
    "print(f\"At x=0.0: True={true_boundary[0]:.4f}, NW={nw_boundary[0]:.4f} (bias={abs(nw_boundary[0]-true_boundary[0]):.4f}), LP={lp_boundary[0]:.4f} (bias={abs(lp_boundary[0]-true_boundary[0]):.4f})\")\n",
    "print(f\"At x=1.0: True={true_boundary[1]:.4f}, NW={nw_boundary[1]:.4f} (bias={abs(nw_boundary[1]-true_boundary[1]):.4f}), LP={lp_boundary[1]:.4f} (bias={abs(lp_boundary[1]-true_boundary[1]):.4f})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bandwidth Selection\n",
    "\n",
    "The bandwidth parameter controls the trade-off between bias and variance. Too small causes overfitting (high variance), too large causes underfitting (high bias). Cross-validation finds the optimal bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate data\n",
    "np.random.seed(99)\n",
    "X = np.linspace(0, 2*np.pi, 60).reshape(-1, 1)\n",
    "y_true = np.sin(X.ravel())\n",
    "y = y_true + 0.3 * np.random.randn(60)\n",
    "\n",
    "# Try different bandwidths\n",
    "bandwidths = [0.1, 0.3, 0.6, 1.0]\n",
    "X_plot = np.linspace(0, 2*np.pi, 300).reshape(-1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, h in enumerate(bandwidths):\n",
    "    model = NadarayaWatson(bandwidth=h).fit(X, y)\n",
    "    y_pred = model.predict(X_plot)\n",
    "    \n",
    "    axes[i].scatter(X, y, alpha=0.5, s=30, c='gray', label='Data')\n",
    "    axes[i].plot(X_plot, np.sin(X_plot.ravel()), 'g--', linewidth=2, label='True', alpha=0.7)\n",
    "    axes[i].plot(X_plot, y_pred, 'r-', linewidth=2.5, label=f'Prediction')\n",
    "    axes[i].set_xlabel('X', fontsize=11)\n",
    "    axes[i].set_ylabel('y', fontsize=11)\n",
    "    axes[i].set_title(f'Bandwidth h = {h}', fontsize=12, fontweight='bold')\n",
    "    axes[i].legend(fontsize=10)\n",
    "    axes[i].grid(alpha=0.3)\n",
    "    \n",
    "    # Add annotation\n",
    "    if i == 0:\n",
    "        axes[i].text(0.5, 0.95, 'Too small: Overfitting', transform=axes[i].transAxes,\n",
    "                    fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    elif i == 3:\n",
    "        axes[i].text(0.5, 0.95, 'Too large: Underfitting', transform=axes[i].transAxes,\n",
    "                    fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now use cross-validation\n",
    "print(\"\\nUsing Cross-Validation to select optimal bandwidth...\")\n",
    "model_cv = NadarayaWatson(bandwidth=\"cv\").fit(X, y)\n",
    "print(f\"Optimal bandwidth: {model_cv.bandwidth_[0]:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Dimension Bandwidth (Variable Selection)\n",
    "\n",
    "When dealing with multivariate data, different features may need different amounts of smoothing. Per-dimension bandwidth selection can identify and smooth out irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate data: X1 is signal, X2 is noise\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "X1 = np.linspace(0, 4*np.pi, n)  # Signal feature\n",
    "X2 = np.random.randn(n) * 3      # Noise feature\n",
    "X = np.column_stack([X1, X2])\n",
    "y = np.sin(X1) + 0.2 * np.random.randn(n)  # Only depends on X1\n",
    "\n",
    "# Fit with per-dimension bandwidth\n",
    "selector = CrossValidatedBandwidth(cv=\"loo\", per_dimension=True)\n",
    "bandwidth = selector(X, y)\n",
    "\n",
    "print(\"Per-Dimension Bandwidth Selection:\")\n",
    "print(f\"  Signal feature (X1) bandwidth: {bandwidth[0]:.4f}\")\n",
    "print(f\"  Noise feature (X2) bandwidth:  {bandwidth[1]:.4f}\")\n",
    "print(f\"  Ratio (noise/signal): {bandwidth[1]/bandwidth[0]:.1f}x\")\n",
    "print(\"\\n✓ Large bandwidth for X2 means it gets smoothed out (automatic variable selection)\")\n",
    "\n",
    "# Visualize the data\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# 3D scatter\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "scatter = ax1.scatter(X1, X2, y, c=y, cmap='viridis', s=30, alpha=0.6)\n",
    "ax1.set_xlabel('X1 (Signal)', fontsize=11)\n",
    "ax1.set_ylabel('X2 (Noise)', fontsize=11)\n",
    "ax1.set_zlabel('y', fontsize=11)\n",
    "ax1.set_title('3D View: y depends only on X1', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax1, pad=0.1, shrink=0.6)\n",
    "\n",
    "# 2D projection\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.scatter(X1, y, alpha=0.5, s=30, c='blue', label='Data')\n",
    "# Fit model and show prediction\n",
    "model = NadarayaWatson(bandwidth=bandwidth).fit(X, y)\n",
    "X1_plot = np.linspace(0, 4*np.pi, 300)\n",
    "X2_mean = np.zeros(300)  # Use mean of X2\n",
    "X_plot = np.column_stack([X1_plot, X2_mean])\n",
    "y_pred = model.predict(X_plot)\n",
    "ax2.plot(X1_plot, y_pred, 'r-', linewidth=2.5, label='Model prediction')\n",
    "ax2.plot(X1_plot, np.sin(X1_plot), 'g--', linewidth=2, label='True function', alpha=0.7)\n",
    "ax2.set_xlabel('X1 (Signal)', fontsize=11)\n",
    "ax2.set_ylabel('y', fontsize=11)\n",
    "ax2.set_title('Model correctly focuses on X1', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Heteroscedasticity Detection\n",
    "\n",
    "Heteroscedasticity occurs when the variance of errors changes across the input space. This package provides multiple tests to detect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate heteroscedastic data (variance increases with x)\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 150).reshape(-1, 1)\n",
    "y_true = 2 * np.sin(X.ravel())\n",
    "# Variance increases with x (\"trumpet\" pattern)\n",
    "noise_std = 0.2 + 0.4 * X.ravel()\n",
    "y = y_true + np.random.randn(150) * noise_std\n",
    "\n",
    "# Fit model\n",
    "model = NadarayaWatson(bandwidth=\"silverman\").fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "residuals = y - y_pred\n",
    "\n",
    "# Test for heteroscedasticity\n",
    "tests = [\"white\", \"breusch_pagan\", \"dette_munk_wagner\"]\n",
    "results = {}\n",
    "for test in tests:\n",
    "    result = heteroscedasticity_test(model, X, y, test=test, alpha=0.05)\n",
    "    results[test] = result\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Data and fit\n",
    "axes[0].scatter(X, y, alpha=0.5, s=30, c='gray', label='Data')\n",
    "axes[0].plot(X, y_true, 'g--', linewidth=2, label='True function', alpha=0.7)\n",
    "axes[0].plot(X, y_pred, 'r-', linewidth=2.5, label='Fitted model')\n",
    "axes[0].set_xlabel('X', fontsize=12)\n",
    "axes[0].set_ylabel('y', fontsize=12)\n",
    "axes[0].set_title('Heteroscedastic Data (Variance Increases)', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals showing increasing variance\n",
    "axes[1].scatter(X, residuals, alpha=0.5, s=30, c='blue')\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "# Add envelope showing standard deviation\n",
    "axes[1].fill_between(X.ravel(), -noise_std, noise_std, alpha=0.2, color='red', label='True σ(x)')\n",
    "axes[1].set_xlabel('X', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[1].set_title('Residual Plot: Variance Increases with X', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "print(\"\\nHeteroscedasticity Test Results:\")\n",
    "print(\"=\"*60)\n",
    "for test, result in results.items():\n",
    "    status = \"DETECTED\" if result.is_heteroscedastic else \"NOT DETECTED\"\n",
    "    print(f\"{test:20s}: p-value = {result.p_value:.4f} [{status}]\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confidence Intervals\n",
    "\n",
    "Wild bootstrap confidence intervals provide robust uncertainty quantification, especially for heteroscedastic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 2*np.pi, 50).reshape(-1, 1)\n",
    "y_true = np.sin(X.ravel())\n",
    "y = y_true + 0.3 * np.random.randn(50)\n",
    "\n",
    "# Fit model\n",
    "model = NadarayaWatson(bandwidth=0.5).fit(X, y)\n",
    "\n",
    "# Generate confidence intervals\n",
    "X_plot = np.linspace(0, 2*np.pi, 100).reshape(-1, 1)\n",
    "ci = wild_bootstrap_confidence_intervals(\n",
    "    model, X, y,\n",
    "    X_pred=X_plot,\n",
    "    confidence_level=0.95,\n",
    "    n_bootstrap=500,\n",
    "    distribution=\"rademacher\",\n",
    "    bias_correction=\"rbc_studentized\",\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.fill_between(X_plot.ravel(), ci.lower, ci.upper, alpha=0.3, color='red', label='95% Confidence Interval')\n",
    "plt.plot(X_plot, ci.predictions, 'r-', linewidth=2.5, label='Prediction')\n",
    "plt.plot(X_plot, np.sin(X_plot.ravel()), 'g--', linewidth=2, label='True function', alpha=0.7)\n",
    "plt.scatter(X, y, alpha=0.6, s=40, c='blue', edgecolors='black', linewidth=0.5, label='Training data')\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Wild Bootstrap Confidence Intervals (RBC Studentized)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confidence level: {ci.confidence_level*100}%\")\n",
    "print(f\"Number of bootstrap samples: {ci.n_bootstrap}\")\n",
    "print(f\"Bias correction method: {ci.bias_correction}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Goodness of Fit Diagnostics\n",
    "\n",
    "The package provides comprehensive goodness-of-fit metrics including R², adjusted R², AIC, BIC, and effective degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 2*np.pi, 100).reshape(-1, 1)\n",
    "y_true = np.sin(X.ravel())\n",
    "y = y_true + 0.3 * np.random.randn(100)\n",
    "\n",
    "# Fit model\n",
    "model = NadarayaWatson(bandwidth=\"cv\").fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Get goodness of fit\n",
    "gof = GoodnessOfFit(model, X, y)\n",
    "\n",
    "# Print summary\n",
    "print(\"Goodness of Fit Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(gof.summary())\n",
    "\n",
    "# Visualize diagnostics\n",
    "residuals = y - y_pred\n",
    "leverage = gof.get_leverage_values()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Fitted vs Actual\n",
    "axes[0, 0].scatter(y, y_pred, alpha=0.5, s=30)\n",
    "min_val = min(y.min(), y_pred.min())\n",
    "max_val = max(y.max(), y_pred.max())\n",
    "axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Actual y', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Predicted y', fontsize=11)\n",
    "axes[0, 0].set_title(f'Fitted vs Actual (R² = {gof.r_squared:.4f})', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals vs Fitted\n",
    "axes[0, 1].scatter(y_pred, residuals, alpha=0.5, s=30)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Fitted values', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Residuals', fontsize=11)\n",
    "axes[0, 1].set_title('Residuals vs Fitted', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot (Normality Check)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Leverage\n",
    "axes[1, 1].scatter(range(len(leverage)), leverage, alpha=0.5, s=30)\n",
    "axes[1, 1].axhline(y=2*gof.effective_df/len(y), color='r', linestyle='--', linewidth=2, label='2*p/n threshold')\n",
    "axes[1, 1].set_xlabel('Observation Index', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Leverage', fontsize=11)\n",
    "axes[1, 1].set_title(f'Leverage Plot (Effective DF = {gof.effective_df:.2f})', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 2D Regression Visualization\n",
    "\n",
    "Kernel regression naturally extends to multiple dimensions. Here we visualize a 2D regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate 2D data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "X1 = np.random.uniform(-3, 3, n)\n",
    "X2 = np.random.uniform(-3, 3, n)\n",
    "X = np.column_stack([X1, X2])\n",
    "\n",
    "# True function: z = sin(x1) * cos(x2)\n",
    "z_true = np.sin(X1) * np.cos(X2)\n",
    "z = z_true + 0.3 * np.random.randn(n)\n",
    "\n",
    "# Fit model\n",
    "model = NadarayaWatson(bandwidth=\"silverman\").fit(X, z)\n",
    "\n",
    "# Create prediction grid\n",
    "grid_size = 50\n",
    "x1_grid = np.linspace(-3, 3, grid_size)\n",
    "x2_grid = np.linspace(-3, 3, grid_size)\n",
    "X1_grid, X2_grid = np.meshgrid(x1_grid, x2_grid)\n",
    "X_grid = np.column_stack([X1_grid.ravel(), X2_grid.ravel()])\n",
    "z_pred = model.predict(X_grid).reshape(grid_size, grid_size)\n",
    "z_true_grid = (np.sin(X1_grid) * np.cos(X2_grid))\n",
    "\n",
    "# Create figure with subplots\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot 1: True function\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf1 = ax1.plot_surface(X1_grid, X2_grid, z_true_grid, cmap='viridis', alpha=0.7, edgecolor='none')\n",
    "ax1.set_xlabel('X1', fontsize=10)\n",
    "ax1.set_ylabel('X2', fontsize=10)\n",
    "ax1.set_zlabel('z', fontsize=10)\n",
    "ax1.set_title('True Function\\nz = sin(x₁) × cos(x₂)', fontsize=12, fontweight='bold')\n",
    "fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=5)\n",
    "\n",
    "# Plot 2: Training data\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "scatter = ax2.scatter(X1, X2, z, c=z, cmap='viridis', s=20, alpha=0.6)\n",
    "ax2.set_xlabel('X1', fontsize=10)\n",
    "ax2.set_ylabel('X2', fontsize=10)\n",
    "ax2.set_zlabel('z', fontsize=10)\n",
    "ax2.set_title(f'Training Data\\n(n={n} points)', fontsize=12, fontweight='bold')\n",
    "fig.colorbar(scatter, ax=ax2, shrink=0.5, aspect=5)\n",
    "\n",
    "# Plot 3: Predicted surface\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "surf3 = ax3.plot_surface(X1_grid, X2_grid, z_pred, cmap='viridis', alpha=0.7, edgecolor='none')\n",
    "ax3.scatter(X1, X2, z, c='red', s=10, alpha=0.3)\n",
    "ax3.set_xlabel('X1', fontsize=10)\n",
    "ax3.set_ylabel('X2', fontsize=10)\n",
    "ax3.set_zlabel('z', fontsize=10)\n",
    "ax3.set_title('Kernel Regression Fit\\nNadaraya-Watson', fontsize=12, fontweight='bold')\n",
    "fig.colorbar(surf3, ax=ax3, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute fit quality\n",
    "z_pred_train = model.predict(X)\n",
    "mse = np.mean((z - z_pred_train) ** 2)\n",
    "r2 = 1 - np.sum((z - z_pred_train) ** 2) / np.sum((z - np.mean(z)) ** 2)\n",
    "print(f\"\\nFit Quality:\")\n",
    "print(f\"  MSE: {mse:.4f}\")\n",
    "print(f\"  R²: {r2:.4f}\")\n",
    "print(f\"  Bandwidth: [{model.bandwidth_[0]:.3f}, {model.bandwidth_[1]:.3f}]\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the key features of the `kernel-regression` package:\n",
    "\n",
    "1. **Nadaraya-Watson Regression**: Basic kernel regression with automatic bandwidth selection\n",
    "2. **Local Polynomial Regression**: Higher-order local fits for complex patterns\n",
    "3. **Boundary Bias**: How local polynomial regression eliminates boundary bias\n",
    "4. **Bandwidth Selection**: The bias-variance tradeoff and cross-validation\n",
    "5. **Variable Selection**: Per-dimension bandwidth for automatic feature selection\n",
    "6. **Heteroscedasticity**: Detection of non-constant variance\n",
    "7. **Confidence Intervals**: Wild bootstrap for robust uncertainty quantification\n",
    "8. **Diagnostics**: Comprehensive goodness-of-fit metrics\n",
    "9. **2D Regression**: Extension to multivariate problems\n",
    "\n",
    "For more information, see the [README](../README.md) and the package documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
